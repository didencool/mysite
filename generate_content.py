import datetime
import torch
import requests
from bs4 import BeautifulSoup
# –Ø–∫—â–æ –≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç–µ Hugging Face:
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# 1. –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø
# =========================================================
# –í–ö–ê–ó–ê–ù–ù–Ø CPU –û–ë–û–í'–Ø–ó–ö–û–í–ï!
device = torch.device('cpu') 
MODEL_NAME = "—Ç—É—Ç/–≤–∞—à–∞-–Ω–∞–∑–≤–∞-–º–æ–¥–µ–ª—ñ-–¥–ª—è-–ø—ñ–¥—Å—É–º–∫—ñ–≤" 
NEWS_URL = "https://example.com/some/news/feed"

def fetch_and_summarize():
    """–í–∏–∫–æ–Ω—É—î –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–∏–Ω —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é –ø—ñ–¥—Å—É–º–∫—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é PyTorch."""
    
    # 1. –ó–ë–Ü–† –î–ê–ù–ò–• (–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–∏–Ω)
    try:
        response = requests.get(NEWS_URL, timeout=10)
        response.raise_for_status() # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–æ–º–∏–ª–æ–∫ HTTP
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # üö® –í–∞—à–∞ –ª–æ–≥—ñ–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É: –∑–Ω–∞–π–¥—ñ—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞ —Ç–µ–∫—Å—Ç –Ω–æ–≤–∏–Ω
        articles = []
        for item in soup.find_all('article', limit=5): # –ü–∞—Ä—Å–∏–º–æ, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 5 —Å—Ç–∞—Ç–µ–π
            title = item.find('h2').text
            text = item.find('p').text 
            articles.append({'title': title, 'text': text})
            
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–±–æ—Ä—ñ –Ω–æ–≤–∏–Ω: {e}")
        return "<p>–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ –Ω–æ–≤–∏–Ω.</p>"
        
    # 2. –û–ë–†–û–ë–ö–ê PYTORCH (–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—ñ–¥—Å—É–º–∫—ñ–≤)
    summaries = []
    try:
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ CPU
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)
        
        for article in articles:
            # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç—É –¥–ª—è –º–æ–¥–µ–ª—ñ
            inputs = tokenizer(article['text'], return_tensors="pt", max_length=512, truncation=True).to(device)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—ñ–¥—Å—É–º–∫—É (–Ω–∞ CPU)
            summary_ids = model.generate(
                inputs.input_ids, 
                max_length=50, 
                min_length=10,
                num_beams=2,
                do_sample=False
            )
            summary_text = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)
            
            summaries.append(f"<li><strong>{article['title']}</strong>: {summary_text}</li>")

    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ä–æ–±–æ—Ç—ñ PyTorch: {e}")
        return "<p>–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ä–æ–±–æ—Ç—ñ –º–æ–¥–µ–ª—ñ –ø—ñ–¥—Å—É–º–æ–≤—É–≤–∞–Ω–Ω—è.</p>"

    return "\n".join(summaries)


# 3. –§–û–†–ú–£–í–ê–ù–ù–Ø –§–Ü–ù–ê–õ–¨–ù–û–ì–û HTML
# =========================================================
if __name__ == "__main__":
    
    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫—ñ–≤ —É —Ñ–æ—Ä–º–∞—Ç—ñ HTML-—Å–ø–∏—Å–∫—É
    summaries_html = fetch_and_summarize() 
    
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # –§–æ—Ä–º—É–≤–∞–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ HTML-—Ñ–∞–π–ª—É
    final_html_content = f"""
<!DOCTYPE html>
<html lang="uk">
<head>
    <meta charset="UTF-8">
    <title>–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –ó–≤—ñ—Ç –ù–æ–≤–∏–Ω</title>
    </head>
<body>
    <div class="container">
        <h1>üì∞ –û–Ω–æ–≤–ª–µ–Ω–æ PyTorch-–º–æ–¥–µ–ª–ª—é</h1>
        <div class="content-section" id="news-summary">
            <h2>–û—Å—Ç–∞–Ω–Ω—ñ –ü—ñ–¥—Å—É–º–∫–∏</h2>
            <ul>
                {summaries_html}
            </ul>
        </div>
        <p class="update-info">
            –û—Å—Ç–∞–Ω–Ω—î –æ–Ω–æ–≤–ª–µ–Ω–Ω—è: <strong>{current_time}</strong>
        </p>
    </div>
</body>
</html>
"""

    # –ü–µ—Ä–µ–∑–∞–ø–∏—Å index.html
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(final_html_content)
    
    print(f"Content successfully generated and index.html updated at {current_time}")
